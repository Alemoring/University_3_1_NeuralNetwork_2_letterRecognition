{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3481d40-a3c2-4ad4-9a36-779231b4d674",
      "metadata": {
        "id": "a3481d40-a3c2-4ad4-9a36-779231b4d674"
      },
      "source": [
        "# File \"Network\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f3713c3e-977c-49c4-9713-e9672756d78a",
      "metadata": {
        "id": "f3713c3e-977c-49c4-9713-e9672756d78a",
        "outputId": "9cb8d698-10e9-4fd2-f53f-929da070fdad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сеть net:\n",
            "Количетво слоев: 3\n",
            "Количество нейронов в слое 0 : 2\n",
            "Количество нейронов в слое 1 : 3\n",
            "Количество нейронов в слое 2 : 1\n",
            "W_ 1 :\n",
            "[[-0.79 -0.42]\n",
            " [-2.36  0.72]\n",
            " [-0.84 -0.27]]\n",
            "b_ 1 :\n",
            "[[0.66]\n",
            " [0.33]\n",
            " [0.9 ]]\n",
            "W_ 2 :\n",
            "[[ 0.71 -0.36  0.58]]\n",
            "b_ 2 :\n",
            "[[-1.07]]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "network.py\n",
        "Модуль создания и обучения нейронной сети для распознавания рукописных цифр\n",
        "с использованием метода градиентного спуска.\n",
        "Группа:ИСТб-22-1\n",
        "ФИО:Моргунов Александр Алексеевич\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "#### Библиотеки\n",
        "# Стандартные библиотеки\n",
        "import random # библиотека функций для генерации случайных значений\n",
        "# Сторонние библиотеки\n",
        "import numpy as np # библиотека функций для работы с матрицами\n",
        "\"\"\" ---Раздел описаний--- \"\"\"\n",
        "\"\"\" --Описание класса Network--\"\"\"\n",
        "class Network(object): # используется для описания нейронной сети\n",
        "    def __init__(self, sizes): # конструктор класса\n",
        "        # self – указатель на объект класса\n",
        "        # sizes – список размеров слоев нейронной сети\n",
        "        self.num_layers = len(sizes) # задаем количество слоев нейронной сети\n",
        "        self.sizes = sizes # задаем список размеров слоев нейронной сети\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # задаем случайные начальные смещения\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1],sizes[1:])] # задаем случайные начальные веса связей\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    def SGD( # Стохастический градиентный спуск\n",
        "    self # указатель на объект класса\n",
        "    , training_data # обучающая выборка\n",
        "    , epochs # количество эпох обучения\n",
        "    , mini_batch_size # размер подвыборки\n",
        "    , eta # скорость обучения\n",
        "    , test_data # тестирующая выборка\n",
        "    ):\n",
        "        test_data = list(test_data) # создаем список объектов тестирующей выборки\n",
        "        n_test = len(test_data) # вычисляем длину тестирующей выборки\n",
        "        training_data = list(training_data) # создаем список объектов обучающей выборки\n",
        "        n = len(training_data) # вычисляем размер обучающей выборки\n",
        "        for j in range(epochs): # цикл по эпохам\n",
        "            random.shuffle(training_data) # перемешиваем элементы обучающей выборки\n",
        "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] # создаем подвыборки\n",
        "            for mini_batch in mini_batches: # цикл по подвыборкам\n",
        "                self.update_mini_batch(mini_batch, eta) # один шаг градиентного спуска\n",
        "            print (\"Epoch {0}: {1} / {2} = {3}\".format(j, self.evaluate(test_data), n_test, self.evaluate(test_data)/n_test)) # смотрим прогресс в обучении\n",
        "    def update_mini_batch( # Шаг градиентного спуска\n",
        "        self # указатель на объект класса\n",
        "        , mini_batch # подвыборка\n",
        "        , eta # скорость обучения\n",
        "        ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # послойно вычисляем градиенты dC/db и dC/dw для текущего прецедента (x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # суммируем градиенты dC/db для различных прецедентов текущей подвыборки\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # суммируем градиенты dC/dw для различных прецедентов текущей подвыборки\n",
        "            self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] # обновляем все веса w нейронной сети\n",
        "            self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] # обновляем все смещения b нейронной сети\n",
        "    def backprop( # Алгоритм обратного распространения\n",
        "        self # указатель на объект класса\n",
        "        , x # вектор входных сигналов\n",
        "        , y # ожидаемый вектор выходных сигналов\n",
        "        ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "        # определение переменных\n",
        "        activation = x # выходные сигналы слоя (первоначально соответствует выходным сигналам 1-го слоя или входным сигналам сети)\n",
        "        activations = [x] # список выходных сигналов по всем слоям (первоначально содержит только выходные сигналы 1-го слоя)\n",
        "        zs = [] # список активационных потенциалов по всем слоям (первоначально пуст)\n",
        "\n",
        "        # прямое распространение\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b # считаем активационные потенциалы текущего слоя\n",
        "            zs.append(z) # добавляем элемент (активационные потенциалы слоя) в конец списка\n",
        "            activation = sigmoid(z) # считаем выходные сигналы текущего слоя, применяя сигмоидальную функцию активации к активационным потенциалам слоя\n",
        "            activations.append(activation) # добавляем элемент (выходные сигналы слоя) в конец списка\n",
        "        # обратное распространение\n",
        "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # считаем меру влияния нейронов выходного слоя L на величину ошибки (BP1)\n",
        "        nabla_b[-1] = delta # градиент dC/db для слоя L (BP3)\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # градиент dC/dw для слоя L (BP4)\n",
        "\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l] # активационные потенциалы l-го слоя (двигаемся по списку справа налево)\n",
        "            sp = sigmoid_prime(z) # считаем сигмоидальную функцию от активационных потенциалов l-го слоя\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp # считаем меру влияния нейронов l-го слоя на величину ошибки (BP2)\n",
        "            nabla_b[-l] = delta # градиент dC/db для l-го слоя (BP3)\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())# градиент dC/dw для l-го слоя (BP4)\n",
        "        return (nabla_b, nabla_w)\n",
        "    def evaluate(self, test_data): # Оценка прогресса в обучении\n",
        "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
        "        return sum(int(x == y) for (x, y) in test_results)\n",
        "    def cost_derivative(self, output_activations, y): # Вычисление частных производных стоимостной функции по выходным сигналам последнего слоя\n",
        "        return (output_activations-y)\n",
        "\"\"\" --Конец описания класса Network--\"\"\"\n",
        "\"\"\" --- Конец раздела описаний--- \"\"\"\n",
        "\"\"\" ---Тело программы--- \"\"\"\n",
        "net = Network([2, 3, 1]) # создаем нейронную сеть из трех слоев\n",
        "\"\"\" ---Конец тела программы--- \"\"\"\n",
        "\"\"\" Вывод результата на экран: \"\"\"\n",
        "print('Сеть net:')\n",
        "print('Количетво слоев:', net.num_layers)\n",
        "for i in range(net.num_layers):\n",
        "    print('Количество нейронов в слое', i,':',net.sizes[i])\n",
        "for i in range(net.num_layers-1):\n",
        "    print('W_',i+1,':')\n",
        "    print(np.round(net.weights[i],2))\n",
        "    print('b_',i+1,':')\n",
        "    print(np.round(net.biases[i],2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1ade7d5c-50f9-40dc-9cdb-65d0039b8226",
      "metadata": {
        "id": "1ade7d5c-50f9-40dc-9cdb-65d0039b8226"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z): # определение сигмоидальной функции активации\n",
        "    return 1.0/(1.0+np.exp(-z))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "42613d1e-808c-4264-844e-3e54f402854d",
      "metadata": {
        "id": "42613d1e-808c-4264-844e-3e54f402854d"
      },
      "outputs": [],
      "source": [
        "def sigmoid_prime(z):# Производная сигмоидальной функции\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299ed519-6d4c-4b62-ba8a-9b074890cce6",
      "metadata": {
        "id": "299ed519-6d4c-4b62-ba8a-9b074890cce6"
      },
      "source": [
        "# File \"MNIST loader\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "22e1ac87-6ca5-4a5c-abd6-751a4902b2e1",
      "metadata": {
        "id": "22e1ac87-6ca5-4a5c-abd6-751a4902b2e1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "mnist_loader.py\n",
        "~~~~~~~~~~\n",
        "Модуль для подключения и использования базы данных MNIST.\n",
        "Группа:ИСТб-22-1\n",
        "ФИО:Моргунов Александр Алексеевич\n",
        "\"\"\"\n",
        "import gzip # библиотека для сжатия и распаковки файлов gzip и gunzip.\n",
        "import pickle # библиотека для сохранения и загрузки сложных объектов Python.\n",
        "import numpy as np # библиотека для работы с матрицами\n",
        "def load_data():\n",
        "    f = gzip.open('mnist.pkl.gz', 'rb') # открываем сжатый файл gzip в двоичном режиме\n",
        "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1') # загружам таблицы из файла\n",
        "    f.close() # закрываем файл\n",
        "    return (training_data, validation_data, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9d4a2b7d-c10b-46b7-ba82-c7e5959b1e73",
      "metadata": {
        "id": "9d4a2b7d-c10b-46b7-ba82-c7e5959b1e73"
      },
      "outputs": [],
      "source": [
        "def load_data_wrapper():\n",
        "    tr_d, va_d, te_d = load_data() # инициализация наборов данных в формате MNIST\n",
        "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
        "    training_results = [vectorized_result(y) for y in tr_d[1]] # представление цифр от 0 до 9 в виде массивов размера 10 на 1\n",
        "    training_data = zip(training_inputs, training_results) # формируем набор обучающих данных из пар (x, y)\n",
        "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
        "    validation_data = zip(validation_inputs, va_d[1]) # формируем набор данных проверки из пар (x, y)\n",
        "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
        "    test_data = zip(test_inputs, te_d[1]) # формируем набор тестовых данных из пар (x, y)\n",
        "    return (training_data, validation_data, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f89b006f-f0bb-4e41-8a83-6884b75be797",
      "metadata": {
        "id": "f89b006f-f0bb-4e41-8a83-6884b75be797"
      },
      "outputs": [],
      "source": [
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac458205-84a2-4ed1-89cd-e7aa5b3cd619",
      "metadata": {
        "id": "ac458205-84a2-4ed1-89cd-e7aa5b3cd619"
      },
      "source": [
        "# Interpretator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ac5315b7-29f9-434a-ac5b-2e84f941f7aa",
      "metadata": {
        "id": "ac5315b7-29f9-434a-ac5b-2e84f941f7aa"
      },
      "outputs": [],
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bd64eb2b-8260-4dc7-b167-666cd731ff8a",
      "metadata": {
        "id": "bd64eb2b-8260-4dc7-b167-666cd731ff8a"
      },
      "outputs": [],
      "source": [
        "net = Network([784, 30, 10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "78ead7a3-1889-41b2-98c7-c9a03496477d",
      "metadata": {
        "id": "78ead7a3-1889-41b2-98c7-c9a03496477d",
        "outputId": "c14034fd-1373-4f63-acce-0a33d0aeeef6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 8618 / 10000 = 0.8618\n",
            "Epoch 1: 8689 / 10000 = 0.8689\n",
            "Epoch 2: 8823 / 10000 = 0.8823\n",
            "Epoch 3: 8903 / 10000 = 0.8903\n",
            "Epoch 4: 9037 / 10000 = 0.9037\n",
            "Epoch 5: 9026 / 10000 = 0.9026\n",
            "Epoch 6: 9087 / 10000 = 0.9087\n",
            "Epoch 7: 9081 / 10000 = 0.9081\n",
            "Epoch 8: 9087 / 10000 = 0.9087\n",
            "Epoch 9: 9094 / 10000 = 0.9094\n",
            "Epoch 10: 9127 / 10000 = 0.9127\n",
            "Epoch 11: 9137 / 10000 = 0.9137\n",
            "Epoch 12: 9184 / 10000 = 0.9184\n",
            "Epoch 13: 9153 / 10000 = 0.9153\n",
            "Epoch 14: 9153 / 10000 = 0.9153\n",
            "Epoch 15: 9208 / 10000 = 0.9208\n",
            "Epoch 16: 9119 / 10000 = 0.9119\n",
            "Epoch 17: 9260 / 10000 = 0.926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e278208ea042>:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0/(1.0+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: 9185 / 10000 = 0.9185\n",
            "Epoch 19: 9175 / 10000 = 0.9175\n",
            "Epoch 20: 9173 / 10000 = 0.9173\n",
            "Epoch 21: 9149 / 10000 = 0.9149\n",
            "Epoch 22: 9107 / 10000 = 0.9107\n",
            "Epoch 23: 9212 / 10000 = 0.9212\n",
            "Epoch 24: 9249 / 10000 = 0.9249\n",
            "Epoch 25: 9293 / 10000 = 0.9293\n",
            "Epoch 26: 9263 / 10000 = 0.9263\n",
            "Epoch 27: 9176 / 10000 = 0.9176\n",
            "Epoch 28: 9164 / 10000 = 0.9164\n",
            "Epoch 29: 9281 / 10000 = 0.9281\n"
          ]
        }
      ],
      "source": [
        "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba79847c-39bd-406c-b297-1a025ca0ddb5",
      "metadata": {
        "id": "ba79847c-39bd-406c-b297-1a025ca0ddb5"
      },
      "source": [
        "# File \"Network 2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e16a4796-fade-4278-9270-f06f56e9a4c2",
      "metadata": {
        "id": "e16a4796-fade-4278-9270-f06f56e9a4c2",
        "outputId": "94bbfeb7-2cb7-408e-c3e6-ef16e035d8dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' --- Конец раздела описаний--- '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "network2.py\n",
        "Модуль создания и обучения нейронной сети для распознавания рукописных цифр\n",
        "на основе метода стохастического градиентного спуска для прямой нейронной\n",
        "сети и стоимостной функции на основе перекрестной энтропии, регуляризации и\n",
        "улучшеннного способа инициализации весов нейронной сети.\n",
        "Группа: ИСТб-22-1\n",
        "ФИО: Моргунов Александр Алексеевич\n",
        "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
        "#### Библиотеки\n",
        "# Стандартные библиотеки\n",
        "import json # библиотека для кодирования/декодирования данных/объектов Python\n",
        "import random # библиотека функций для генерации случайных значений\n",
        "import sys # библиотека для работы с переменными и функциями, имеющими отношение к интерпретатору и его окружению\n",
        "# Сторонние библиотеки\n",
        "import numpy as np # библиотека функций для работы с матрицами\n",
        "\n",
        "\"\"\" ---Раздел описаний--- \"\"\"\n",
        "\n",
        "\"\"\" -- Определение стоимостных функции --\"\"\"\n",
        "class QuadraticCost(object): # Определение среднеквадратичной стоимостной функции\n",
        "    @staticmethod\n",
        "    def fn(a, y): # Cтоимостная функция\n",
        "        return 0.5*np.linalg.norm(a-y)**2\n",
        "    @staticmethod\n",
        "    def delta(z, a, y): # Мера влияния нейронов выходного слоя на величину ошибки\n",
        "        return (a-y) * sigmoid_prime(z)\n",
        "\"\"\" -- Конец описания стоимостной функции -- \"\"\"\n",
        "\"\"\" -- Описание класса стоимостной функции на основе перекрестной энтропии -- \"\"\"\n",
        "class CrossEntropyCost(object): # Определение стоимостной функции на основе перекрестной энтропии\n",
        "    @staticmethod\n",
        "    def fn(a, y): # Cтоимостная функция\n",
        "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
        "    @staticmethod\n",
        "    def delta(z, a, y): # Мера влияния нейронов выходного слоя на величину ошибки\n",
        "        return (a-y)\n",
        "\"\"\" -- Конец описания класса стоимостной функции на основе перекрестной энтропии -- \"\"\"\n",
        "\"\"\" --Описание класса Network-- \"\"\"\n",
        "class Network2(object):\n",
        "    def __init__( # конструктор класса\n",
        "    self # указатель на объект класса\n",
        "    , sizes # список размеров слоев нейронной сети\n",
        "    , cost=CrossEntropyCost # стоимостная функция (по умолчанию будет использоваться функция перекрестной энтропии)\n",
        "    ):\n",
        "        self.num_layers = len(sizes) # задаем количество слоев нейронной сети\n",
        "        self.sizes = sizes # задаем список размеров слоев нейронной сети\n",
        "        self.default_weight_initializer() # метод инициализации начальных весов связей и смещений по умолчанию\n",
        "        self.cost=cost # задаем стоимостную функцию\n",
        "    def default_weight_initializer(self): # метод инициализации начальных весов связей и смещений\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] # задаем случайные начальные смещения\n",
        "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] # задаем случайные начальные веса связей\n",
        "    def large_weight_initializer(self):\n",
        "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] # задаем случайные начальные смещения\n",
        "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] # задаем случайные начальные веса\n",
        "    def feedforward(self, a):\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
        "            lmbda=0.0  # параметр сглаживания L2-регуляризации\n",
        "            , evaluation_data=None  # оценочная выборка\n",
        "            , monitor_evaluation_cost=False\n",
        "            # флаг вывода на экран информации о значении стоимостной функции в процессе обучения, рассчитанном на оценочной выборке\n",
        "            , monitor_evaluation_accuracy=False\n",
        "            # флаг вывода на экран информации о достигнутом прогрессе в обучении, рассчитанном на оценочной выборке\n",
        "            , monitor_training_cost=False\n",
        "            # флаг вывода на экран информации о значении стоимостной функции в процессе обучения, рассчитанном на обучающей выборке\n",
        "            , monitor_training_accuracy=False\n",
        "            # флаг вывода на экран информации о достигнутом прогрессе в обучении, рассчитанном на обучающей выборке\n",
        "            ):\n",
        "        if evaluation_data:\n",
        "            evaluation_data = list(evaluation_data)\n",
        "            n_data = len(evaluation_data)\n",
        "        training_data = list(training_data)\n",
        "        n = len(training_data)\n",
        "        evaluation_cost, evaluation_accuracy = [], []\n",
        "        training_cost, training_accuracy = [], []\n",
        "        for j in range(epochs):\n",
        "            random.shuffle(training_data)\n",
        "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "            for mini_batch in mini_batches:\n",
        "                self.update_mini_batch(mini_batch, eta, lmbda, len(training_data))\n",
        "            print(\"Epoch %s training complete\" % j)\n",
        "            if monitor_training_cost:\n",
        "                cost = self.total_cost(training_data, lmbda)\n",
        "                training_cost.append(cost)\n",
        "                print(\"--Cost on training data: {}\".format(cost))\n",
        "            if monitor_training_accuracy:\n",
        "                accuracy = self.accuracy(training_data, convert=True)\n",
        "                training_accuracy.append(accuracy)\n",
        "                print(\"--Accuracy on training data: {} / {}\".format(accuracy, n))\n",
        "            if monitor_evaluation_cost:\n",
        "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
        "                evaluation_cost.append(cost)\n",
        "                print(\"--Cost on evaluation data: {}\".format(cost))\n",
        "            if monitor_evaluation_accuracy:\n",
        "                accuracy = self.accuracy(evaluation_data)\n",
        "                evaluation_accuracy.append(accuracy)\n",
        "                print(\"--Accuracy on evaluation data: {} / {}\".format(self.accuracy(evaluation_data), n_data))\n",
        "            print\n",
        "        return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
        "    def update_mini_batch( # Шаг градиентного спуска\n",
        "            self # указатель на объект класса\n",
        "            , mini_batch # подвыборка\n",
        "            , eta # скорость обучения\n",
        "            , lmbda # параметр сглаживания L2-регуляризации\n",
        "            , n #\n",
        "            ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "        for x, y in mini_batch:\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # послойно вычисляем градиенты dC/db и dC/dw для текущего прецедента (x, y)\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # суммируем градиенты dC/db для различных прецедентов текущей подвыборки\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # суммируем градиенты dC/dw для различных прецедентов текущей подвыборки\n",
        "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] # обновляем все веса w нейронной сети\n",
        "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] # обновляем все смещения b нейронной сети\n",
        "    def backprop(# Алгоритм обратного распространения\n",
        "            self # Указатель на объект класса\n",
        "            , x # Вектор входных сигналов\n",
        "            , y # Ожидаемый вектор выходных сигналов\n",
        "            ):\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
        "\n",
        "        # Определение переменных\n",
        "        activation = x # Выходные сигналы слоя (первоначально соответствует выходным сигналам 1-го слоя или входным сигналам сети)\n",
        "        activations = [x] # Список выходных сигналов по всем слоям (первоначально содержит только выходные сигналы 1-го слоя)\n",
        "        zs = [] # Список активационных потенциалов по всем слоям (первоначально пуст)\n",
        "\n",
        "        # Прямое распространение\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            z = np.dot(w, activation)+b # Считаем активационные потенциалы текущего слоя\n",
        "            zs.append(z) # Добавляем элемент (активационные потенциалы слоя) в конец списка\n",
        "            activation = sigmoid(z) # Считаем выходные сигналы текущего слоя, применяя сигмоидальную функцию активации к активационным потенциалам слоя\n",
        "            activations.append(activation) # Добавляем элемент (выходные сигналы слоя) в конец списка\n",
        "        # Обратное распространение\n",
        "        delta = (self.cost).delta(zs[-1], activations[-1], y) # Считаем меру влияния нейронов выходного слоя L на величину ошибки (BP1)\n",
        "        nabla_b[-1] = delta # Градиент dC/db для слоя L (BP3)\n",
        "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())# Градиент dC/dw для слоя L (BP4)\n",
        "        for l in range(2, self.num_layers):\n",
        "            z = zs[-l] # Активационные потенциалы l-го слоя (двигаемся по списку справа налево)\n",
        "            sp = sigmoid_prime(z) # Считаем сигмоидальную функцию от активационных потенциалов l-го слоя\n",
        "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp # Считаем меру влияния нейронов l-го слоя на величину ошибки (BP2)\n",
        "            nabla_b[-l] = delta # Градиент dC/db для l-го слоя (BP3)\n",
        "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        return (nabla_b, nabla_w) # Градиент dC/dw для l-го слоя (BP4)\n",
        "    def accuracy(# Оценка прогресса в обучении\n",
        "         self # Указатель на объект класса\n",
        "         , data # Набор данных (выборка)\n",
        "         , convert=False # Признак необходимости изменять формат представления результата работы нейронной сети\n",
        "         ):\n",
        "         if convert:\n",
        "            results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in data]\n",
        "         else:\n",
        "            results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
        "         return sum(int(x == y) for (x, y) in results)\n",
        "    def total_cost(# Значение функции потерь по всей выборке\n",
        "         self # Указатель на объект класса\n",
        "         , data # Набор данных (выборка)\n",
        "         , lmbda # Параметр сглаживания L2-регуляризации\n",
        "         , convert=False # Признак необходимости изменять формат представления результата работы нейронной сети\n",
        "         ):\n",
        "         cost = 0.0\n",
        "         data = list(data)\n",
        "         for x, y in data:\n",
        "            a = self.feedforward(x)\n",
        "            if convert: y = vectorized_result(y)\n",
        "            cost += self.cost.fn(a, y)/len(data)\n",
        "         cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
        "         return cost\n",
        "    def save(self, filename): # Запись нейронной сети в файл\n",
        "         data = {\"sizes\": self.sizes,\n",
        "                \"weights\": [w.tolist() for w in self.weights],\n",
        "                \"biases\": [b.tolist() for b in self.biases],\n",
        "                \"cost\": str(self.cost.__name__)}\n",
        "         f = open(filename, \"w\")\n",
        "         json.dump(data, f)\n",
        "         f.close()\n",
        "\n",
        "\"\"\" -- Конец описания класса Network -- \"\"\"\n",
        "def load(filename): # Загрузка нейронной сети из файла\n",
        "\n",
        "    f = open(filename, \"r\")\n",
        "    data = json.load(f)\n",
        "    f.close()\n",
        "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
        "    net = Network(data[\"sizes\"], cost=cost)\n",
        "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
        "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
        "    return net\n",
        "def sigmoid(z): # Определение сигмоидальной функции активации\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "def sigmoid_prime(z): # Производная сигмоидальной функции\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "def vectorized_result(j):\n",
        "    e = np.zeros((10, 1))\n",
        "    e[j] = 1.0\n",
        "    return e\n",
        "\n",
        "\"\"\" --- Конец раздела описаний--- \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpretator"
      ],
      "metadata": {
        "id": "0iqPkYLoDL8R"
      },
      "id": "0iqPkYLoDL8R"
    },
    {
      "cell_type": "code",
      "source": [
        "training_data, validation_data, test_data = load_data_wrapper()"
      ],
      "metadata": {
        "id": "sem1belnDQND"
      },
      "id": "sem1belnDQND",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Network2([784, 30, 10], cost=CrossEntropyCost)"
      ],
      "metadata": {
        "id": "P-dfZj0HDhYj"
      },
      "id": "P-dfZj0HDhYj",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net.SGD(training_data, 30, 10, 0.5, lmbda = 5.0,evaluation_data=validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=True, monitor_training_accuracy=True, monitor_training_cost=True)"
      ],
      "metadata": {
        "id": "3s75IS9wDno_",
        "outputId": "e29d2593-1b84-453b-cfd4-c061a71a5b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3s75IS9wDno_",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 training complete\n",
            "--Cost on training data: 0.463420752251909\n",
            "--Accuracy on training data: 47103 / 50000\n",
            "--Cost on evaluation data: 0.7591302633617236\n",
            "--Accuracy on evaluation data: 9445 / 10000\n",
            "Epoch 1 training complete\n",
            "--Cost on training data: 0.4194434980167929\n",
            "--Accuracy on training data: 47701 / 50000\n",
            "--Cost on evaluation data: 0.8312544650908236\n",
            "--Accuracy on evaluation data: 9495 / 10000\n",
            "Epoch 2 training complete\n",
            "--Cost on training data: 0.41198150874197426\n",
            "--Accuracy on training data: 47855 / 50000\n",
            "--Cost on evaluation data: 0.865935508737345\n",
            "--Accuracy on evaluation data: 9558 / 10000\n",
            "Epoch 3 training complete\n",
            "--Cost on training data: 0.39198199314506665\n",
            "--Accuracy on training data: 48082 / 50000\n",
            "--Cost on evaluation data: 0.8803700808712751\n",
            "--Accuracy on evaluation data: 9589 / 10000\n",
            "Epoch 4 training complete\n",
            "--Cost on training data: 0.3944074126739984\n",
            "--Accuracy on training data: 48061 / 50000\n",
            "--Cost on evaluation data: 0.9041516138840344\n",
            "--Accuracy on evaluation data: 9584 / 10000\n",
            "Epoch 5 training complete\n",
            "--Cost on training data: 0.38006586021756467\n",
            "--Accuracy on training data: 48213 / 50000\n",
            "--Cost on evaluation data: 0.9043646882267737\n",
            "--Accuracy on evaluation data: 9604 / 10000\n",
            "Epoch 6 training complete\n",
            "--Cost on training data: 0.4310253451616639\n",
            "--Accuracy on training data: 47985 / 50000\n",
            "--Cost on evaluation data: 0.9653866630444148\n",
            "--Accuracy on evaluation data: 9536 / 10000\n",
            "Epoch 7 training complete\n",
            "--Cost on training data: 0.3741500858183743\n",
            "--Accuracy on training data: 48346 / 50000\n",
            "--Cost on evaluation data: 0.9171035996957538\n",
            "--Accuracy on evaluation data: 9611 / 10000\n",
            "Epoch 8 training complete\n",
            "--Cost on training data: 0.40551625419244475\n",
            "--Accuracy on training data: 48052 / 50000\n",
            "--Cost on evaluation data: 0.9501967334167409\n",
            "--Accuracy on evaluation data: 9541 / 10000\n",
            "Epoch 9 training complete\n",
            "--Cost on training data: 0.36592064068060703\n",
            "--Accuracy on training data: 48368 / 50000\n",
            "--Cost on evaluation data: 0.9177882997816438\n",
            "--Accuracy on evaluation data: 9627 / 10000\n",
            "Epoch 10 training complete\n",
            "--Cost on training data: 0.39134930476760577\n",
            "--Accuracy on training data: 48197 / 50000\n",
            "--Cost on evaluation data: 0.942083814700518\n",
            "--Accuracy on evaluation data: 9587 / 10000\n",
            "Epoch 11 training complete\n",
            "--Cost on training data: 0.3829296666925778\n",
            "--Accuracy on training data: 48282 / 50000\n",
            "--Cost on evaluation data: 0.9326628553167097\n",
            "--Accuracy on evaluation data: 9619 / 10000\n",
            "Epoch 12 training complete\n",
            "--Cost on training data: 0.38386587062845956\n",
            "--Accuracy on training data: 48256 / 50000\n",
            "--Cost on evaluation data: 0.9376210971151016\n",
            "--Accuracy on evaluation data: 9610 / 10000\n",
            "Epoch 13 training complete\n",
            "--Cost on training data: 0.38151813633382303\n",
            "--Accuracy on training data: 48331 / 50000\n",
            "--Cost on evaluation data: 0.9417219160552714\n",
            "--Accuracy on evaluation data: 9598 / 10000\n",
            "Epoch 14 training complete\n",
            "--Cost on training data: 0.4008280198437745\n",
            "--Accuracy on training data: 48034 / 50000\n",
            "--Cost on evaluation data: 0.9583067826095925\n",
            "--Accuracy on evaluation data: 9556 / 10000\n",
            "Epoch 15 training complete\n",
            "--Cost on training data: 0.4086280200081158\n",
            "--Accuracy on training data: 48054 / 50000\n",
            "--Cost on evaluation data: 0.9697314444358518\n",
            "--Accuracy on evaluation data: 9567 / 10000\n",
            "Epoch 16 training complete\n",
            "--Cost on training data: 0.3780904652878845\n",
            "--Accuracy on training data: 48427 / 50000\n",
            "--Cost on evaluation data: 0.9356510906489506\n",
            "--Accuracy on evaluation data: 9601 / 10000\n",
            "Epoch 17 training complete\n",
            "--Cost on training data: 0.38921151658799524\n",
            "--Accuracy on training data: 48246 / 50000\n",
            "--Cost on evaluation data: 0.9446781840355303\n",
            "--Accuracy on evaluation data: 9606 / 10000\n",
            "Epoch 18 training complete\n",
            "--Cost on training data: 0.37704210454916554\n",
            "--Accuracy on training data: 48318 / 50000\n",
            "--Cost on evaluation data: 0.9370497756031929\n",
            "--Accuracy on evaluation data: 9626 / 10000\n",
            "Epoch 19 training complete\n",
            "--Cost on training data: 0.3881868334881909\n",
            "--Accuracy on training data: 48221 / 50000\n",
            "--Cost on evaluation data: 0.9628887277197204\n",
            "--Accuracy on evaluation data: 9579 / 10000\n",
            "Epoch 20 training complete\n",
            "--Cost on training data: 0.3912767774314623\n",
            "--Accuracy on training data: 48198 / 50000\n",
            "--Cost on evaluation data: 0.9534697715495775\n",
            "--Accuracy on evaluation data: 9576 / 10000\n",
            "Epoch 21 training complete\n",
            "--Cost on training data: 0.3622724259285359\n",
            "--Accuracy on training data: 48508 / 50000\n",
            "--Cost on evaluation data: 0.9332354878949352\n",
            "--Accuracy on evaluation data: 9646 / 10000\n",
            "Epoch 22 training complete\n",
            "--Cost on training data: 0.3765030751765127\n",
            "--Accuracy on training data: 48341 / 50000\n",
            "--Cost on evaluation data: 0.9457037525470118\n",
            "--Accuracy on evaluation data: 9639 / 10000\n",
            "Epoch 23 training complete\n",
            "--Cost on training data: 0.38173603509538895\n",
            "--Accuracy on training data: 48285 / 50000\n",
            "--Cost on evaluation data: 0.9548897703766235\n",
            "--Accuracy on evaluation data: 9586 / 10000\n",
            "Epoch 24 training complete\n",
            "--Cost on training data: 0.3765499879228091\n",
            "--Accuracy on training data: 48337 / 50000\n",
            "--Cost on evaluation data: 0.9489669571804961\n",
            "--Accuracy on evaluation data: 9606 / 10000\n",
            "Epoch 25 training complete\n",
            "--Cost on training data: 0.36974915399420144\n",
            "--Accuracy on training data: 48430 / 50000\n",
            "--Cost on evaluation data: 0.9370657386464198\n",
            "--Accuracy on evaluation data: 9636 / 10000\n",
            "Epoch 26 training complete\n",
            "--Cost on training data: 0.4127943439462186\n",
            "--Accuracy on training data: 48013 / 50000\n",
            "--Cost on evaluation data: 0.979625681467867\n",
            "--Accuracy on evaluation data: 9538 / 10000\n",
            "Epoch 27 training complete\n",
            "--Cost on training data: 0.375774972939394\n",
            "--Accuracy on training data: 48299 / 50000\n",
            "--Cost on evaluation data: 0.9523932331235556\n",
            "--Accuracy on evaluation data: 9610 / 10000\n",
            "Epoch 28 training complete\n",
            "--Cost on training data: 0.37025066540571405\n",
            "--Accuracy on training data: 48338 / 50000\n",
            "--Cost on evaluation data: 0.9441994152192288\n",
            "--Accuracy on evaluation data: 9613 / 10000\n",
            "Epoch 29 training complete\n",
            "--Cost on training data: 0.371604701991196\n",
            "--Accuracy on training data: 48338 / 50000\n",
            "--Cost on evaluation data: 0.9442486609386401\n",
            "--Accuracy on evaluation data: 9624 / 10000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.7591302633617236,\n",
              "  0.8312544650908236,\n",
              "  0.865935508737345,\n",
              "  0.8803700808712751,\n",
              "  0.9041516138840344,\n",
              "  0.9043646882267737,\n",
              "  0.9653866630444148,\n",
              "  0.9171035996957538,\n",
              "  0.9501967334167409,\n",
              "  0.9177882997816438,\n",
              "  0.942083814700518,\n",
              "  0.9326628553167097,\n",
              "  0.9376210971151016,\n",
              "  0.9417219160552714,\n",
              "  0.9583067826095925,\n",
              "  0.9697314444358518,\n",
              "  0.9356510906489506,\n",
              "  0.9446781840355303,\n",
              "  0.9370497756031929,\n",
              "  0.9628887277197204,\n",
              "  0.9534697715495775,\n",
              "  0.9332354878949352,\n",
              "  0.9457037525470118,\n",
              "  0.9548897703766235,\n",
              "  0.9489669571804961,\n",
              "  0.9370657386464198,\n",
              "  0.979625681467867,\n",
              "  0.9523932331235556,\n",
              "  0.9441994152192288,\n",
              "  0.9442486609386401],\n",
              " [9445,\n",
              "  9495,\n",
              "  9558,\n",
              "  9589,\n",
              "  9584,\n",
              "  9604,\n",
              "  9536,\n",
              "  9611,\n",
              "  9541,\n",
              "  9627,\n",
              "  9587,\n",
              "  9619,\n",
              "  9610,\n",
              "  9598,\n",
              "  9556,\n",
              "  9567,\n",
              "  9601,\n",
              "  9606,\n",
              "  9626,\n",
              "  9579,\n",
              "  9576,\n",
              "  9646,\n",
              "  9639,\n",
              "  9586,\n",
              "  9606,\n",
              "  9636,\n",
              "  9538,\n",
              "  9610,\n",
              "  9613,\n",
              "  9624],\n",
              " [0.463420752251909,\n",
              "  0.4194434980167929,\n",
              "  0.41198150874197426,\n",
              "  0.39198199314506665,\n",
              "  0.3944074126739984,\n",
              "  0.38006586021756467,\n",
              "  0.4310253451616639,\n",
              "  0.3741500858183743,\n",
              "  0.40551625419244475,\n",
              "  0.36592064068060703,\n",
              "  0.39134930476760577,\n",
              "  0.3829296666925778,\n",
              "  0.38386587062845956,\n",
              "  0.38151813633382303,\n",
              "  0.4008280198437745,\n",
              "  0.4086280200081158,\n",
              "  0.3780904652878845,\n",
              "  0.38921151658799524,\n",
              "  0.37704210454916554,\n",
              "  0.3881868334881909,\n",
              "  0.3912767774314623,\n",
              "  0.3622724259285359,\n",
              "  0.3765030751765127,\n",
              "  0.38173603509538895,\n",
              "  0.3765499879228091,\n",
              "  0.36974915399420144,\n",
              "  0.4127943439462186,\n",
              "  0.375774972939394,\n",
              "  0.37025066540571405,\n",
              "  0.371604701991196],\n",
              " [47103,\n",
              "  47701,\n",
              "  47855,\n",
              "  48082,\n",
              "  48061,\n",
              "  48213,\n",
              "  47985,\n",
              "  48346,\n",
              "  48052,\n",
              "  48368,\n",
              "  48197,\n",
              "  48282,\n",
              "  48256,\n",
              "  48331,\n",
              "  48034,\n",
              "  48054,\n",
              "  48427,\n",
              "  48246,\n",
              "  48318,\n",
              "  48221,\n",
              "  48198,\n",
              "  48508,\n",
              "  48341,\n",
              "  48285,\n",
              "  48337,\n",
              "  48430,\n",
              "  48013,\n",
              "  48299,\n",
              "  48338,\n",
              "  48338])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}