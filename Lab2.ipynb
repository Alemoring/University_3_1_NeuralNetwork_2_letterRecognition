{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3481d40-a3c2-4ad4-9a36-779231b4d674",
   "metadata": {},
   "source": [
    "# File \"Network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3713c3e-977c-49c4-9713-e9672756d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сеть net:\n",
      "Количетво слоев: 3\n",
      "Количество нейронов в слое 0 : 2\n",
      "Количество нейронов в слое 1 : 3\n",
      "Количество нейронов в слое 2 : 1\n",
      "W_ 1 :\n",
      "[[-1.07 -1.18]\n",
      " [-0.07  1.6 ]\n",
      " [-0.62  0.32]]\n",
      "b_ 1 :\n",
      "[[ 1.47]\n",
      " [-1.49]\n",
      " [-2.4 ]]\n",
      "W_ 2 :\n",
      "[[ 0.27 -1.3  -0.21]]\n",
      "b_ 2 :\n",
      "[[1.18]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "network.py\n",
    "Модуль создания и обучения нейронной сети для распознавания рукописных цифр\n",
    "с использованием метода градиентного спуска.\n",
    "Группа:ИСТб-22-1\n",
    "ФИО:Моргунов Александр Алексеевич\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#### Библиотеки\n",
    "# Стандартные библиотеки\n",
    "import random # библиотека функций для генерации случайных значений\n",
    "# Сторонние библиотеки\n",
    "import numpy as np # библиотека функций для работы с матрицами\n",
    "\"\"\" ---Раздел описаний--- \"\"\"\n",
    "\"\"\" --Описание класса Network--\"\"\"\n",
    "class Network(object): # используется для описания нейронной сети\n",
    "    def __init__(self, sizes): # конструктор класса\n",
    "        # self – указатель на объект класса\n",
    "        # sizes – список размеров слоев нейронной сети\n",
    "        self.num_layers = len(sizes) # задаем количество слоев нейронной сети\n",
    "        self.sizes = sizes # задаем список размеров слоев нейронной сети\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]] # задаем случайные начальные смещения\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1],sizes[1:])] # задаем случайные начальные веса связей\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    def SGD( # Стохастический градиентный спуск\n",
    "    self # указатель на объект класса\n",
    "    , training_data # обучающая выборка\n",
    "    , epochs # количество эпох обучения\n",
    "    , mini_batch_size # размер подвыборки\n",
    "    , eta # скорость обучения\n",
    "    , test_data # тестирующая выборка\n",
    "    ):\n",
    "        test_data = list(test_data) # создаем список объектов тестирующей выборки\n",
    "        n_test = len(test_data) # вычисляем длину тестирующей выборки\n",
    "        training_data = list(training_data) # создаем список объектов обучающей выборки\n",
    "        n = len(training_data) # вычисляем размер обучающей выборки\n",
    "        for j in range(epochs): # цикл по эпохам\n",
    "            random.shuffle(training_data) # перемешиваем элементы обучающей выборки\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)] # создаем подвыборки\n",
    "            for mini_batch in mini_batches: # цикл по подвыборкам\n",
    "                self.update_mini_batch(mini_batch, eta) # один шаг градиентного спуска\n",
    "            print (\"Epoch {0}: {1} / {2} = {3}\".format(j, self.evaluate(test_data), n_test, self.evaluate(test_data)/n_test)) # смотрим прогресс в обучении\n",
    "    def update_mini_batch( # Шаг градиентного спуска\n",
    "        self # указатель на объект класса\n",
    "        , mini_batch # подвыборка\n",
    "        , eta # скорость обучения\n",
    "        ):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # послойно вычисляем градиенты dC/db и dC/dw для текущего прецедента (x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # суммируем градиенты dC/db для различных прецедентов текущей подвыборки\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # суммируем градиенты dC/dw для различных прецедентов текущей подвыборки\n",
    "            self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] # обновляем все веса w нейронной сети\n",
    "            self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] # обновляем все смещения b нейронной сети\n",
    "    def backprop( # Алгоритм обратного распространения\n",
    "        self # указатель на объект класса\n",
    "        , x # вектор входных сигналов\n",
    "        , y # ожидаемый вектор выходных сигналов\n",
    "        ):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # список градиентов dC/db для каждого слоя (первоначально заполняются нулями)\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] # список градиентов dC/dw для каждого слоя (первоначально заполняются нулями)\n",
    "        # определение переменных\n",
    "        activation = x # выходные сигналы слоя (первоначально соответствует выходным сигналам 1-го слоя или входным сигналам сети)\n",
    "        activations = [x] # список выходных сигналов по всем слоям (первоначально содержит только выходные сигналы 1-го слоя)\n",
    "        zs = [] # список активационных потенциалов по всем слоям (первоначально пуст)\n",
    "\n",
    "        # прямое распространение\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b # считаем активационные потенциалы текущего слоя\n",
    "            zs.append(z) # добавляем элемент (активационные потенциалы слоя) в конец списка\n",
    "            activation = sigmoid(z) # считаем выходные сигналы текущего слоя, применяя сигмоидальную функцию активации к активационным потенциалам слоя\n",
    "            activations.append(activation) # добавляем элемент (выходные сигналы слоя) в конец списка \n",
    "        # обратное распространение\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # считаем меру влияния нейронов выходного слоя L на величину ошибки (BP1)\n",
    "        nabla_b[-1] = delta # градиент dC/db для слоя L (BP3)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # градиент dC/dw для слоя L (BP4)\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] # активационные потенциалы l-го слоя (двигаемся по списку справа налево)\n",
    "            sp = sigmoid_prime(z) # считаем сигмоидальную функцию от активационных потенциалов l-го слоя\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp # считаем меру влияния нейронов l-го слоя на величину ошибки (BP2)\n",
    "            nabla_b[-l] = delta # градиент dC/db для l-го слоя (BP3)\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())# градиент dC/dw для l-го слоя (BP4)\n",
    "        return (nabla_b, nabla_w)\n",
    "    def evaluate(self, test_data): # Оценка прогресса в обучении\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    def cost_derivative(self, output_activations, y): # Вычисление частных производных стоимостной функции по выходным сигналам последнего слоя\n",
    "        return (output_activations-y)\n",
    "\"\"\" --Конец описания класса Network--\"\"\"\n",
    "\"\"\" --- Конец раздела описаний--- \"\"\"\n",
    "\"\"\" ---Тело программы--- \"\"\"\n",
    "net = Network([2, 3, 1]) # создаем нейронную сеть из трех слоев\n",
    "\"\"\" ---Конец тела программы--- \"\"\"\n",
    "\"\"\" Вывод результата на экран: \"\"\"\n",
    "print('Сеть net:')\n",
    "print('Количетво слоев:', net.num_layers)\n",
    "for i in range(net.num_layers):\n",
    "    print('Количество нейронов в слое', i,':',net.sizes[i])\n",
    "for i in range(net.num_layers-1):\n",
    "    print('W_',i+1,':')\n",
    "    print(np.round(net.weights[i],2))\n",
    "    print('b_',i+1,':')\n",
    "    print(np.round(net.biases[i],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ade7d5c-50f9-40dc-9cdb-65d0039b8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z): # определение сигмоидальной функции активации\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42613d1e-808c-4264-844e-3e54f402854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):# Производная сигмоидальной функции\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ed519-6d4c-4b62-ba8a-9b074890cce6",
   "metadata": {},
   "source": [
    "# File \"MNIST loader\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22e1ac87-6ca5-4a5c-abd6-751a4902b2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader.py\n",
    "~~~~~~~~~~\n",
    "Модуль для подключения и использования базы данных MNIST.\n",
    "Группа:ИСТб-22-1\n",
    "ФИО:Моргунов Александр Алексеевич\n",
    "\"\"\"\n",
    "import gzip # библиотека для сжатия и распаковки файлов gzip и gunzip.\n",
    "import pickle # библиотека для сохранения и загрузки сложных объектов Python.\n",
    "import numpy as np # библиотека для работы с матрицами\n",
    "def load_data():\n",
    "    f = gzip.open('mnist.pkl.gz', 'rb') # открываем сжатый файл gzip в двоичном режиме\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='latin1') # загружам таблицы из файла\n",
    "    f.close() # закрываем файл\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d4a2b7d-c10b-46b7-ba82-c7e5959b1e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data() # инициализация наборов данных в формате MNIST\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]] # представление цифр от 0 до 9 в виде массивов размера 10 на 1\n",
    "    training_data = zip(training_inputs, training_results) # формируем набор обучающих данных из пар (x, y)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
    "    validation_data = zip(validation_inputs, va_d[1]) # формируем набор данных проверки из пар (x, y)\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]] # преобразование массивов размера 1 на 784 к массивам размера 784 на 1\n",
    "    test_data = zip(test_inputs, te_d[1]) # формируем набор тестовых данных из пар (x, y)\n",
    "    return (training_data, validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f89b006f-f0bb-4e41-8a83-6884b75be797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac458205-84a2-4ed1-89cd-e7aa5b3cd619",
   "metadata": {},
   "source": [
    "# Interpretator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac5315b7-29f9-434a-ac5b-2e84f941f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd64eb2b-8260-4dc7-b167-666cd731ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78ead7a3-1889-41b2-98c7-c9a03496477d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 7575 / 10000 = 0.7575\n",
      "Epoch 1: 7471 / 10000 = 0.7471\n",
      "Epoch 2: 8680 / 10000 = 0.868\n",
      "Epoch 3: 8537 / 10000 = 0.8537\n",
      "Epoch 4: 8687 / 10000 = 0.8687\n",
      "Epoch 5: 8541 / 10000 = 0.8541\n",
      "Epoch 6: 8810 / 10000 = 0.881\n",
      "Epoch 7: 8673 / 10000 = 0.8673\n",
      "Epoch 8: 8860 / 10000 = 0.886\n",
      "Epoch 9: 8643 / 10000 = 0.8643\n",
      "Epoch 10: 8763 / 10000 = 0.8763\n",
      "Epoch 11: 8939 / 10000 = 0.8939\n",
      "Epoch 12: 8816 / 10000 = 0.8816\n",
      "Epoch 13: 8842 / 10000 = 0.8842\n",
      "Epoch 14: 8797 / 10000 = 0.8797\n",
      "Epoch 15: 8721 / 10000 = 0.8721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morgu\\AppData\\Local\\Temp\\ipykernel_9260\\3245411285.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0/(1.0+np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 8932 / 10000 = 0.8932\n",
      "Epoch 17: 8924 / 10000 = 0.8924\n",
      "Epoch 18: 8942 / 10000 = 0.8942\n",
      "Epoch 19: 8924 / 10000 = 0.8924\n",
      "Epoch 20: 8919 / 10000 = 0.8919\n",
      "Epoch 21: 8884 / 10000 = 0.8884\n",
      "Epoch 22: 8949 / 10000 = 0.8949\n",
      "Epoch 23: 9019 / 10000 = 0.9019\n",
      "Epoch 24: 8837 / 10000 = 0.8837\n",
      "Epoch 25: 8881 / 10000 = 0.8881\n",
      "Epoch 26: 9079 / 10000 = 0.9079\n",
      "Epoch 27: 9026 / 10000 = 0.9026\n",
      "Epoch 28: 8948 / 10000 = 0.8948\n",
      "Epoch 29: 8862 / 10000 = 0.8862\n"
     ]
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79847c-39bd-406c-b297-1a025ca0ddb5",
   "metadata": {},
   "source": [
    "# File \"Network 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16a4796-fade-4278-9270-f06f56e9a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "network2.py\n",
    "Модуль создания и обучения нейронной сети для распознавания рукописных цифр\n",
    "на основе метода стохастического градиентного спуска для прямой нейронной\n",
    "сети и стоимостной функции на основе перекрестной энтропии, регуляризации и\n",
    "улучшеннного способа инициализации весов нейронной сети.\n",
    "Группа: ИСТб-22-1\n",
    "ФИО: Моргунов Александр Алексеевич\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "#### Библиотеки\n",
    "# Стандартные библиотеки\n",
    "import json # библиотека для кодирования/декодирования данных/объектов Python\n",
    "import random # библиотека функций для генерации случайных значений\n",
    "import sys # библиотека для работы с переменными и функциями, имеющими отношение к интерпретатору и его окружению\n",
    "# Сторонние библиотеки\n",
    "import numpy as np # библиотека функций для работы с матрицами\n",
    "\"\"\" ---Раздел описаний--- \"\"\"\n",
    "\n",
    "\"\"\" -- Определение стоимостных функции --\"\"\"\n",
    "class QuadraticCost(object): # Определение среднеквадратичной стоимостной функции\n",
    "    @staticmethod\n",
    "    def fn(a, y): # Cтоимостная функция\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "    @staticmethod\n",
    "    def delta(z, a, y): # Мера влияния нейронов выходного слоя на величину ошибки\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\"\"\" -- Конец описания стоимостной функции -- \"\"\"\n",
    "\"\"\" -- Описание класса стоимостной функции на основе перекрестной энтропии -- \"\"\"\n",
    "class CrossEntropyCost(object): # Определение стоимостной функции на основе перекрестной энтропии\n",
    "    @staticmethod\n",
    "    def fn(a, y): # Cтоимостная функция\n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "    @staticmethod\n",
    "    def delta(z, a, y): # Мера влияния нейронов выходного слоя на величину ошибки\n",
    "        return (a-y)\n",
    "\"\"\" -- Конец описания класса стоимостной функции на основе перекрестной энтропии -- \"\"\"\n",
    "\"\"\" --Описание класса Network-- \"\"\"\n",
    "class Network(object):\n",
    "    def __init__( # конструктор класса\n",
    "    self # указатель на объект класса\n",
    "    , sizes # список размеров слоев нейронной сети\n",
    "    , cost=CrossEntropyCost # стоимостная функция (по умолчанию будет использоваться функция перекрестной энтропии)\n",
    "    ):\n",
    "    self.num_layers = len(sizes) # задаем количество слоев нейронной сети\n",
    "    self.sizes = sizes # задаем список размеров слоев нейронной сети\n",
    "    self.default_weight_initializer() # метод инициализации начальных весов связей и смещений по умолчанию\n",
    "    self.cost=cost # задаем стоимостную функцию\n",
    "    def default_weight_initializer(self): # метод инициализации начальных весов связей и смещений\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] # задаем случайные начальные смещения\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] # задаем случайные начальные веса связей\n",
    "    def large_weight_initializer(self):\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]] # задаем случайные начальные смещения\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self.sizes[1:])] # задаем случайные начальные веса\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\"\"\" -- Конец описания класса Network -- \"\"\"        \n",
    "\"\"\" --- Конец раздела описаний--- \"\"\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
